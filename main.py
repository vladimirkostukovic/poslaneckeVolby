import requests
from bs4 import BeautifulSoup
import pandas as pd
import time

BASE_URL = "https://www.volby.cz/pls/ps2017nss/"
REGION_URL = "https://www.volby.cz/pls/ps2017nss/ps311?xjazyk=CZ&xkraj={}"  # URL ≈°ablona pro regiony 1-15


# Funkce pro zpracov√°n√≠ obecn√© volebn√≠ statistiky
def parse_general_info(soup, region_id):
    try:
        region_name = soup.find("h3").text.strip().replace("Kraj: ", "")
        table = soup.find("table", {"id": "ps311_t1"})
        rows = table.find_all("tr")[2]  # Bereme 3. ≈ô√°dek s daty

        cols = [td.text.strip().replace("\xa0", "") for td in rows.find_all("td")]

        return {
            "Kraj ID": region_id,
            "Kraj N√°zev": region_name,
            "Okrsky celkem": cols[0],
            "Okrsky zpr": cols[1],
            "Okrsky %": cols[2],
            "Voliƒçi": cols[3],
            "Vydan√© ob√°lky": cols[4],
            "√öƒçast (%)": cols[5],
            "Odevzdan√© ob√°lky": cols[6],
            "Platn√© hlasy": cols[7],
            "Platn√© hlasy (%)": cols[8],
        }
    except Exception as e:
        print(f"‚ùå Chyba p≈ôi zpracov√°n√≠ obecn√© informace pro kraj {region_id}: {e}")
        return None


# Funkce pro zpracov√°n√≠ hlas≈Ø jednotliv√Ωch stran
def parse_party_results(soup, region_id, city_name):
    try:
        table = soup.find_all("table", {"class": "table"})[1]  # Bereme druhou tabulku s v√Ωsledky stran
        rows = table.find_all("tr")[2:]  # P≈ôeskakujeme z√°hlav√≠

        party_results = []
        for row in rows:
            cols = row.find_all("td")
            if len(cols) < 4:
                continue

            party_results.append({
                "Kraj ID": region_id,
                "Mƒõsto": city_name,
                "ƒå√≠slo strany": cols[0].text.strip(),
                "N√°zev strany": cols[1].text.strip(),
                "Hlasy celkem": cols[2].text.strip().replace("\xa0", ""),
                "Hlasy v %": cols[3].text.strip().replace("\xa0", ""),
            })

        return party_results

    except Exception as e:
        print(f"‚ùå Chyba p≈ôi zpracov√°n√≠ v√Ωsledk≈Ø stran pro kraj {region_id}: {e}")
        return []


# Hlavn√≠ funkce pro spu≈°tƒõn√≠ parseru pro regiony (1-15)
def scrape_all_regions():
    all_general_info = []
    all_party_results = []

    for region_id in range(1, 16):  # Smyƒçka p≈ôes regiony 1-15
        print(f"üìç Zpracov√°v√°me kraj {region_id}...")
        url = REGION_URL.format(region_id)
        response = requests.get(url)

        if response.status_code != 200:
            print(f"‚ùå Chyba p≈ôi naƒç√≠t√°n√≠ str√°nky: {url}")
            continue

        soup = BeautifulSoup(response.text, "html.parser")
        city_name = soup.find("h3").text.strip().replace("Kraj: ", "")

        general_info = parse_general_info(soup, region_id)
        party_results = parse_party_results(soup, region_id, city_name)

        if general_info:
            all_general_info.append(general_info)

        if party_results:
            all_party_results.extend(party_results)  # P≈ôid√°v√°me v≈°echny strany do seznamu

        time.sleep(1)  # ‚è≥ Pauza mezi po≈æadavky (aby nedo≈°lo k blokaci)

    # P≈ôepisujeme soubory se z√°hlav√≠m
    pd.DataFrame(all_general_info).to_csv("general_info.csv", index=False, encoding="utf-8", mode='w', header=True)
    pd.DataFrame(all_party_results).to_csv("party_results.csv", index=False, encoding="utf-8", mode='w', header=True)

    print("‚úÖ V≈°echny kraje byly √∫spƒõ≈°nƒõ zpracov√°ny!")


# üî• Spu≈°tƒõn√≠ parseru
scrape_all_regions()